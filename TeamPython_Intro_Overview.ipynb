{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Our Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project we are analyzing the sentiments of different search terms and their related content. A user will be able to compare the emotional, social and language tones of different articles related to their search terms as well as the frequency of those articles. The code to run our project consists of different APIs and functions we will walk you through in our presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a search term is entered it flows through a couple of steps before giving the desired output. The steps are as followed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1)</b> Enter search term or terms into program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2)</b> Algorithm looks for related terms and their relative-frequency as far back as 2004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a) Finds all related articles within most-popular date range\n",
    "    b) Creates range by month for frequency of articles\n",
    "    c) Returns article text by scraping the information embedded in the HTML of the site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3)</b> Takes content from url and passes it into Watson Tone Analyzer\n",
    "    - https://www.ibm.com/watson/developercloud/tone-analyzer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4)</b> Display and compares results from Watson\n",
    "    - https://www.ibm.com/watson/developercloud/doc/tone-analyzer/understanding-tone.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import parse\n",
    "import enchant\n",
    "import json\n",
    "import pandas as pd\n",
    "from watson_developer_cloud import ToneAnalyzerV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GoogleCorrelation(search_term, perc_correlation=0):\n",
    "    \"\"\"\n",
    "    This function take in a search term percent correlation and returns a list of related terms from Google Correlate:\n",
    "    https://www.google.com/trends/correlate\n",
    "    \"\"\"\n",
    "\n",
    "    #format that search term and append it to the Google URL.\n",
    "    search_term_frmt = search_term.replace(' ','+').lower()\n",
    "    url = \"https://www.google.com/trends/correlate/search?e={}&t=weekly&p=us\".format(search_term_frmt)\n",
    "\n",
    "    #Google doesn't allow webscrapers ontheir sites so we need to mask our id (i.e. User Agent) by looking like a regular user.\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "    headers={'User-Agent':user_agent,}\n",
    "    request=urllib.request.Request(url,None,headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    #read the html from the site navigated to by the urllib request.\n",
    "    data = response.read()\n",
    "\n",
    "    #ingest the HTML into bs4 package as a 'soup' object.\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "\n",
    "    #identify the correlated search term object in the HTML \n",
    "    results = soup.find(\"div\", {\"id\":\"results\"})\n",
    "\n",
    "    #find the scores of the correlated search terms\n",
    "    scores = [float(str(x).replace('<small>','').replace('</small>','')) for x in list(results.findAll(\"small\"))]\n",
    "\n",
    "    #create a list of correlated search terms\n",
    "    searches = [(str(results.find(\"span\")).replace('<span>','').replace('</span>',''))]#first search term is formatted differently in HTML\n",
    "    terms_ut = [str(x) for x in results.findAll(\"a\", {\"onclick\":\"addHash(this);\"})]#untrimmed terms via list compresion\n",
    "\n",
    "    #loop through remaining correlated search terms (untrimmed), trim them, and append then to the searches list.\n",
    "    for term in terms_ut:\n",
    "        term_t = term[term.find('\">')+2:term.find('</')]\n",
    "        searches.append(term_t)\n",
    "\n",
    "    #combine the two lists (search_terms, scores) into a list of tuples using the zip function.\n",
    "    corr_searches = [(search_term,1.0)] + list(zip(searches, scores))\n",
    "\n",
    "    #return a list of terms that are 90% similar\n",
    "    return list(filter(lambda x: x[1]>(float(perc_correlation)/100),corr_searches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News Links / Google News Domain / News Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GoogleNewsLinks(search_term, date_start, date_end):\n",
    "    \"\"\"\n",
    "    This function returns a list of articles URLs from the Google News Advanced Search Feature.\n",
    "    \"\"\"\n",
    "\n",
    "    search_term_frmt = '%22'+search_term.replace(' ','+')+'%22'\n",
    "    date_start_frmt = date_start[5:7]+'%2F'+date_start[8:10]+'%2F'+date_start[0:4]\n",
    "    date_end_frmt = date_end[5:7]+'%2F'+date_end[8:10]+'%2F'+date_end[0:4]\n",
    "\n",
    "\n",
    "    url = ('https://www.google.com/search?cf=all&hl=en&pz=1&ned=us&tbm=nws&gl=us&as_epq={exct}&as_occt=any&as_drrb=b&as_mindate={st1}&as_maxdate={end1}&tbs=cdr%3A1%2Ccd_min%3A{st2}%2Ccd_max%3A{end2}&authuser=0&tbas=0'\n",
    "           .format(exct=search_term_frmt,\n",
    "                  st1=date_start_frmt,\n",
    "                  end1=date_end_frmt,\n",
    "                  st2=date_start_frmt,\n",
    "                  end2=date_end_frmt))\n",
    "\n",
    "\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "    headers={'User-Agent':user_agent,}\n",
    "    request=urllib.request.Request(url,None,headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read()\n",
    "\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "\n",
    "    news_articles_ut = list(filter(lambda x: 'href=\"/url?q=' in str(x), list(soup.findAll(\"a\"))))\n",
    "\n",
    "    news_articles = list(set([str(x)[str(x).find('/url?q')+7:str(x).find('&amp')] for x in news_articles_ut]))\n",
    "\n",
    "    return news_articles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetArticleDomain(url):\n",
    "    \"\"\"\n",
    "    This function returns the domain of the website which owns the article.\n",
    "    \"\"\"\n",
    "\n",
    "    return '{uri.scheme}://{uri.netloc}/'.format(uri=parse.urlparse(url))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetArticle(url):\n",
    "    \"\"\"\n",
    "    This function take a URL as input and returns the content contained within the 'p'-tags of the html.\n",
    "    p-tags represent paragraphs on webpages.\n",
    "    \n",
    "    \"\"\"\n",
    "    #send python to the URL and extract the data through the masked user agent \n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "    headers={'User-Agent':user_agent,}\n",
    "    request=urllib.request.Request(url,None,headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read()\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    \n",
    "\n",
    "    #return html contained within 'p-tags' as a list and then concatenate that list seperated with '.'\n",
    "    text_body = \". \".join([str(x) for x in soup.findAll('p')])\n",
    "    \n",
    "    \n",
    "    #replace most non-alpha characters and common tags associated with html\n",
    "    replace_grammar_char = ['0','1','2','3','4','5','6','7','8','9',',',';','?','!','.','+','_','(',')','[',']','{','}','\\n','%','/','#','~','<','>','*','&','=','|','\"','@',':']\n",
    "    replace_html = ['class','div','function','click','amp','sections','quot','com','news','var','head','buffer','script','follow','res','homepage','configuration','wrapper','byline','span','copyright']\n",
    "    replace_terms = replace_grammar_char+replace_html\n",
    "    \n",
    "    for char in replace_terms:\n",
    "        text_body = text_body.replace(char, ' ')\n",
    "    text_body_words = text_body.split(' ')\n",
    "    text_body_words = list(filter(lambda x: x != '' and len(x)>1, text_body_words))\n",
    "     \n",
    "    #import english dictionary for lookup purposes\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    #remove words which are in the top 200 words and are less than 3 charatcers long\n",
    "    relevant_words = list(filter(lambda x: d.check(x) == True, text_body_words))\n",
    "\n",
    "    return \" \".join(relevant_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Frequency and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-4-5daeecfc2147>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-5daeecfc2147>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    return data_points\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "def GetSearchFrequency(search_term):\n",
    "    \"\"\"\n",
    "    This function takes a search term as its input and returns a list of normalized scores per month since 1/1/2004.\n",
    "    Normalized score means that the month which saw the most searches becomes 100, and everything else gets scaled appropriately.\n",
    "    \"\"\"\n",
    "\n",
    "    search = search_term.replace(' ', '+')\n",
    "    url = ('https://www.google.com/trends/fetchComponent?hl=en-US&q={}&cid=TIMESERIES_GRAPH_0&export=5&w=500&h=300'\n",
    "            .format(search))\n",
    "\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "    headers={'User-Agent':user_agent,}\n",
    "    request=urllib.request.Request(url,None,headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read()\n",
    "\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "\n",
    "    graph_date = str(soup.findAll(\"script\", {\"type\":\"text/javascript\"})[3])\n",
    "\n",
    "    json_frmt = graph_date[573:len(graph_date)-386]\n",
    "\n",
    "    split_data = json_frmt.split(',')\n",
    "    data_points = []\n",
    "    i=0\n",
    "        \n",
    "    for element in split_data:\n",
    "        if element.startswith('\"f\":'):\n",
    "            data_points.append([element, split_data[i+3]])\n",
    "        i+=1\n",
    "\n",
    "    data_points = list(map(lambda x: [x[0][5:len(x[0])-2], int(x[1])], data_points))\n",
    "return data_points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetStartEnd(list_obj):\n",
    "    \"\"\"\n",
    "    This fucntion takes the return obect from search term frequency and transforms it.\n",
    "    The return obj is a 3 element list [st_date, end_date, frequency]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    st_mnth = list_obj[0][0:list_obj[0].find(' ')]\n",
    "    \n",
    "    if st_mnth == 'January':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-01-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-02-01'\n",
    "    elif st_mnth == 'February':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-02-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-03-01'\n",
    "    elif st_mnth == 'March':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-03-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-04-01'\n",
    "    elif st_mnth == 'April':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-04-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-05-01'\n",
    "    elif st_mnth == 'May':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-05-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-06-01'\n",
    "    elif st_mnth == 'June':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-06-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-07-01'\n",
    "    elif st_mnth == 'July':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-07-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-08-01'\n",
    "    elif st_mnth == 'August':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-08-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-09-01'\n",
    "    elif st_mnth == 'September':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-09-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-10-01'\n",
    "    elif st_mnth == 'October':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-10-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-11-01'\n",
    "    elif st_mnth == 'November':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-11-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-12-01'\n",
    "    elif st_mnth == 'December':\n",
    "        st = list_obj[0][list_obj[0].find(' ')+1:]+'-12-01'\n",
    "        end = list_obj[0][list_obj[0].find(' ')+1:]+'-12-31'\n",
    "    else:\n",
    "        st = '9999-12-31'\n",
    "        end = '9999-12-31'\n",
    "    \n",
    "    return [st, end, list_obj[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Watson API and format JSON return object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Watson Tone Analyzer API\n",
    "\n",
    "def GetWatsonTones(text_input):\n",
    "    \"\"\"\n",
    "    This function takes a block of text and runs it though the Watson Tone Analyzer API\n",
    "    \"\"\"\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "        username= 'c409ff34-a19a-4b04-a41b-fa79174887ed',\n",
    "        password= '6XZs2TTaCtg3',\n",
    "        version= '2016-05-19 ')\n",
    " \n",
    "    #enter url to be analyze below\n",
    "    d = json.dumps(tone_analyzer.tone(text = text_input), indent=2)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_json(watson):\n",
    "    \"\"\"\n",
    "    This function takes a json str object and converts it to a list of list for sentiment score analysis\n",
    "    \"\"\"\n",
    "    ct_scores = []\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            frmt = [watson['sentences_tone'][0]['tone_categories'][i]['category_name'],\n",
    "                    watson['sentences_tone'][0]['tone_categories'][0]['tones'][j]['tone_name'],\n",
    "                    watson['sentences_tone'][0]['tone_categories'][0]['tones'][j]['score']]\n",
    "\n",
    "            ct_scores.append(frmt)\n",
    "    \n",
    "return ct_scores"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
